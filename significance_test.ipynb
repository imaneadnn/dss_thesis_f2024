{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ground truth\n",
    "y_test = pd.read_csv(\"y_test.csv\")\n",
    "\n",
    "#  predictions\n",
    "y_pred_rfr_imb = pd.read_csv(\"rfr_predictions.csv\")\n",
    "y_pred_gbr_imb = pd.read_csv(\"gbr_predictions.csv\")\n",
    "y_pred_lr_imb = pd.read_csv(\"lr_predictions.csv\")\n",
    "\n",
    "y_pred_rfr_rus = pd.read_csv(\"rfr_rus_predictions.csv\")\n",
    "y_pred_gbr_rus = pd.read_csv(\"gbr_rus_predictions.csv\")\n",
    "y_pred_lr_rus = pd.read_csv(\"lr_rus_predictions.csv\")\n",
    "\n",
    "y_pred_rfr_smogn = pd.read_csv(\"rfr_smogn_predictions.csv\")\n",
    "y_pred_gbr_smogn = pd.read_csv(\"gbr_smogn_predictions.csv\")\n",
    "y_pred_lr_smogn = pd.read_csv(\"lr_smong_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays or ensure numeric data\n",
    "y_test = y_test.squeeze()  # If it's a single column, this converts it to a Series\n",
    "y_pred_rfr_imb = y_pred_rfr_imb.squeeze()\n",
    "y_pred_gbr_imb = y_pred_gbr_imb.squeeze()\n",
    "y_pred_lr_imb = y_pred_lr_imb.squeeze()\n",
    "\n",
    "y_pred_rfr_rus = y_pred_rfr_rus.squeeze()\n",
    "y_pred_gbr_rus = y_pred_gbr_rus.squeeze()\n",
    "y_pred_lr_rus = y_pred_lr_rus.squeeze()\n",
    "\n",
    "y_pred_rfr_smogn = y_pred_rfr_smogn.squeeze()\n",
    "y_pred_gbr_smogn = y_pred_gbr_smogn.squeeze()\n",
    "y_pred_lr_smogn = y_pred_lr_smogn.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    \"Imbalanced\": {\n",
    "        \"RFR\": y_pred_rfr_imb,\n",
    "        \"GBR\": y_pred_gbr_imb,\n",
    "        \"LR\": y_pred_lr_imb\n",
    "    },\n",
    "    \"RUS\": {\n",
    "        \"RFR\": y_pred_rfr_rus,\n",
    "        \"GBR\": y_pred_gbr_rus,\n",
    "        \"LR\": y_pred_lr_rus\n",
    "    },\n",
    "    \"SMOGN\": {\n",
    "        \"RFR\": y_pred_rfr_smogn,\n",
    "        \"GBR\": y_pred_gbr_smogn,\n",
    "        \"LR\": y_pred_lr_smogn\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Imbalanced': {'GBR': {'MAE': 13.72888503372887,\n",
      "                        'RMSE': 23.302560764266183,\n",
      "                        'R^2': 0.6099586232822474},\n",
      "                'LR': {'MAE': 26.43679455599293,\n",
      "                       'RMSE': 33.4735744356499,\n",
      "                       'R^2': 0.19516368612006607},\n",
      "                'RFR': {'MAE': 13.287579646900893,\n",
      "                        'RMSE': 22.8823569344281,\n",
      "                        'R^2': 0.623898648610066}},\n",
      " 'RUS': {'GBR': {'MAE': 14.114070697403195,\n",
      "                 'RMSE': 23.321577915482205,\n",
      "                 'R^2': 0.609321740271259},\n",
      "         'LR': {'MAE': 27.879433715144398,\n",
      "                'RMSE': 33.977483453504995,\n",
      "                'R^2': 0.17074938603390655},\n",
      "         'RFR': {'MAE': 13.836728323449549,\n",
      "                 'RMSE': 23.036119201920332,\n",
      "                 'R^2': 0.6188270996291861}},\n",
      " 'SMOGN': {'GBR': {'MAE': 17.042149431990293,\n",
      "                   'RMSE': 25.923027985679646,\n",
      "                   'R^2': 0.5173027330331613},\n",
      "           'LR': {'MAE': 33.56461132198325,\n",
      "                  'RMSE': 41.00727614168293,\n",
      "                  'R^2': -0.20788399849256467},\n",
      "           'RFR': {'MAE': 17.34821615492378,\n",
      "                   'RMSE': 25.741568602873553,\n",
      "                   'R^2': 0.5240367755561861}}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize an empty dictionary to store metrics\n",
    "metrics = {}\n",
    "\n",
    "# Loop through datasets and models\n",
    "for dataset, models in predictions.items():\n",
    "    metrics[dataset] = {}\n",
    "    for model, y_pred in models.items():\n",
    "        metrics[dataset][model] = {\n",
    "            \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "            \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            \"R^2\": r2_score(y_test, y_pred)\n",
    "        }\n",
    "\n",
    "# Print metrics for verification\n",
    "import pprint\n",
    "pprint.pprint(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bootstrap_errors(y_true, y_pred, n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Generate bootstrap samples of absolute errors.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: Ground truth values.\n",
    "    - y_pred: Predicted values.\n",
    "    - n_bootstrap: Number of bootstrap iterations.\n",
    "    \n",
    "    Returns:\n",
    "    - A NumPy array of bootstrapped mean absolute errors (MAE).\n",
    "    \"\"\"\n",
    "    n_samples = len(y_true)\n",
    "    bootstrap_mae = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Sample indices with replacement\n",
    "        indices = np.random.choice(range(n_samples), size=n_samples, replace=True)\n",
    "        # Calculate MAE for the sampled data\n",
    "        mae = np.mean(np.abs(y_true[indices] - y_pred[indices]))\n",
    "        bootstrap_mae.append(mae)\n",
    "\n",
    "    return np.array(bootstrap_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store bootstrapped errors\n",
    "bootstrap_results = {}\n",
    "\n",
    "for dataset, models in predictions.items():\n",
    "    bootstrap_results[dataset] = {}\n",
    "    for model, y_pred in models.items():\n",
    "        # Perform bootstrapping\n",
    "        bootstrap_results[dataset][model] = bootstrap_errors(y_test.values, y_pred.values, n_bootstrap=1000)\n",
    "\n",
    "# Example output: bootstrap_results['Imbalanced']['RFR'] contains 1000 bootstrapped MAE values for RFR on the imbalanced dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GBR': {'GBR': 'N/A', 'LR': 'Y', 'RFR': 'Y'},\n",
      " 'LR': {'GBR': 'Y', 'LR': 'N/A', 'RFR': 'Y'},\n",
      " 'RFR': {'GBR': 'Y', 'LR': 'Y', 'RFR': 'N/A'}}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def generate_significance_table(bootstrap_results, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Generate a significance table (Y/N) for all model pairs based on bootstrap t-tests.\n",
    "\n",
    "    Parameters:\n",
    "    - bootstrap_results: Dictionary with dataset->model->bootstrapped errors.\n",
    "    - alpha: Significance level (default: 0.05).\n",
    "\n",
    "    Returns:\n",
    "    - A nested dictionary with model pairs and their significance results.\n",
    "    \"\"\"\n",
    "    significance_table = {}\n",
    "\n",
    "    for dataset, models in bootstrap_results.items():\n",
    "        model_names = list(models.keys())\n",
    "        significance_table[dataset] = {}\n",
    "\n",
    "        for i, model1 in enumerate(model_names):\n",
    "            significance_table[dataset][model1] = {}\n",
    "            for j, model2 in enumerate(model_names):\n",
    "                if model1 == model2:\n",
    "                    significance_table[dataset][model1][model2] = \"N/A\"  # Self-comparison\n",
    "                else:\n",
    "                    # Perform t-test\n",
    "                    _, p_value = ttest_ind(bootstrap_results[dataset][model1], bootstrap_results[dataset][model2])\n",
    "                    # Add \"Y\" or \"N\" based on significance\n",
    "                    significance_table[dataset][model1][model2] = \"Y\" if p_value < alpha else \"N\"\n",
    "\n",
    "    return significance_table\n",
    "\n",
    "# Generate significance tables\n",
    "significance_tables = generate_significance_table(bootstrap_results, alpha=0.05)\n",
    "\n",
    "# Example printout for one dataset\n",
    "import pprint\n",
    "pprint.pprint(significance_tables[\"Imbalanced\"])  # Print table for the \"Imbalanced\" dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " & RFR & GBR & LR \\\\\\hline\n",
      "RFR & N/A & Y & Y \\\\\\hline\n",
      "GBR & Y & N/A & Y \\\\\\hline\n",
      "LR & Y & Y & N/A \\\\\\hline\n"
     ]
    }
   ],
   "source": [
    "def print_latex_table(significance_table, dataset_name):\n",
    "    \"\"\"\n",
    "    Print the significance table in LaTeX format.\n",
    "\n",
    "    Parameters:\n",
    "    - significance_table: Nested dictionary with significance results (Y/N).\n",
    "    - dataset_name: Name of the dataset (e.g., \"Imbalanced\").\n",
    "    \"\"\"\n",
    "    table = significance_table[dataset_name]\n",
    "    model_names = list(table.keys())\n",
    "\n",
    "    # Print column headers\n",
    "    print(\" & \" + \" & \".join(model_names) + \" \\\\\\\\\\\\hline\")\n",
    "\n",
    "    # Print each row\n",
    "    for model1 in model_names:\n",
    "        row = model1 + \" & \" + \" & \".join(table[model1].values())\n",
    "        print(row + \" \\\\\\\\\\\\hline\")\n",
    "\n",
    "# Example: Print the table for the \"Imbalanced\" dataset\n",
    "print_latex_table(significance_tables, \"Imbalanced\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def generate_significance_table_across_combinations(bootstrap_results, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Generate a single significance table across all model-dataset combinations.\n",
    "\n",
    "    Parameters:\n",
    "    - bootstrap_results: Dictionary with dataset -> model -> bootstrapped errors.\n",
    "    - alpha: Significance level (default: 0.05).\n",
    "\n",
    "    Returns:\n",
    "    - A nested dictionary with all combinations of model-dataset pairs and their significance results.\n",
    "    \"\"\"\n",
    "    significance_table = {}\n",
    "\n",
    "    # Flatten dataset and model combinations\n",
    "    combinations_list = [\n",
    "        (f\"{dataset}-{model}\", errors)\n",
    "        for dataset, models in bootstrap_results.items()\n",
    "        for model, errors in models.items()\n",
    "    ]\n",
    "    \n",
    "    # Compare all pairs\n",
    "    for (name1, errors1), (name2, errors2) in combinations(combinations_list, 2):\n",
    "        if name1 not in significance_table:\n",
    "            significance_table[name1] = {}\n",
    "        if name2 not in significance_table:\n",
    "            significance_table[name2] = {}\n",
    "\n",
    "        # Perform t-test\n",
    "        _, p_value = ttest_ind(errors1, errors2)\n",
    "        significance_table[name1][name2] = \"Y\" if p_value < alpha else \"N\"\n",
    "        significance_table[name2][name1] = significance_table[name1][name2]\n",
    "\n",
    "    # Add self-comparisons\n",
    "    for name in significance_table:\n",
    "        significance_table[name][name] = \"N/A\"\n",
    "\n",
    "    return significance_table\n",
    "\n",
    "def print_latex_table_unified(significance_table):\n",
    "    \"\"\"\n",
    "    Print a unified LaTeX table for all model-dataset combinations.\n",
    "\n",
    "    Parameters:\n",
    "    - significance_table: Dictionary with significance results for all combinations.\n",
    "    \"\"\"\n",
    "    # Extract all model-dataset pairs\n",
    "    model_dataset_pairs = list(significance_table.keys())\n",
    "\n",
    "    # Print column headers\n",
    "    print(\" & \" + \" & \".join(model_dataset_pairs) + \" \\\\\\\\\\\\hline\")\n",
    "    \n",
    "    # Print each row\n",
    "    for pair1 in model_dataset_pairs:\n",
    "        row = pair1 + \" & \" + \" & \".join(significance_table[pair1][pair2] for pair2 in model_dataset_pairs)\n",
    "        print(row + \" \\\\\\\\\\\\hline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_significance_table = generate_significance_table_across_combinations(bootstrap_results, alpha=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " & Imbalanced-RFR & Imbalanced-GBR & Imbalanced-LR & RUS-RFR & RUS-GBR & RUS-LR & SMOGN-RFR & SMOGN-GBR & SMOGN-LR \\\\\\hline\n",
      "Imbalanced-RFR & N/A & Y & Y & Y & Y & Y & Y & Y & Y \\\\\\hline\n",
      "Imbalanced-GBR & Y & N/A & Y & Y & Y & Y & Y & Y & Y \\\\\\hline\n",
      "Imbalanced-LR & Y & Y & N/A & Y & Y & Y & Y & Y & Y \\\\\\hline\n",
      "RUS-RFR & Y & Y & Y & N/A & Y & Y & Y & Y & Y \\\\\\hline\n",
      "RUS-GBR & Y & Y & Y & Y & N/A & Y & Y & Y & Y \\\\\\hline\n",
      "RUS-LR & Y & Y & Y & Y & Y & N/A & Y & Y & Y \\\\\\hline\n",
      "SMOGN-RFR & Y & Y & Y & Y & Y & Y & N/A & Y & Y \\\\\\hline\n",
      "SMOGN-GBR & Y & Y & Y & Y & Y & Y & Y & N/A & Y \\\\\\hline\n",
      "SMOGN-LR & Y & Y & Y & Y & Y & Y & Y & Y & N/A \\\\\\hline\n"
     ]
    }
   ],
   "source": [
    "print_latex_table_unified(unified_significance_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
